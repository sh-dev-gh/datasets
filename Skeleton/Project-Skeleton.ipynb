{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI614: Data Science & its Applications\n",
    "\n",
    "*Project-Skeleton: DAnalyzing Skiils from Job Postings*\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/harmanani/AAI614/blob/main/Week%207/Project-Skeleton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll begin by loading all the HTML files in the `job_postings` directory.\n",
    "# Loading HTML files**\n",
    "\n",
    "import glob\n",
    "html_contents = []\n",
    "\n",
    "for file_name in sorted(glob.glob('job_postings/*.html')):\n",
    "    with open(file_name, 'r') as f:\n",
    "        html_contents.append(f.read())\n",
    "        \n",
    "print(f\"We've loaded {len(html_contents)} HTML files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've loaded 1458 HTML files into `html_contents`. Each HTML file can be parsed using Beautiful Soup. \n",
    "\n",
    "**Parsing HTML files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIXME by adding the necessary import for the below code to work\n",
    "import #FIXME\n",
    "\n",
    "soup_objects = []\n",
    "for html in html_contents:\n",
    "    soup = #FIXME\n",
    "    assert soup.title is not None\n",
    "    assert soup.body is not None\n",
    "    soup_objects.append(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parsed HTML file contains a title and a body.  Are there any duplicates across the titles or bodies of these files? We can find out by using Pandas.\n",
    "\n",
    "**Checking title and body texts for duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "html_dict = {'Title': [], 'Body': []}\n",
    "\n",
    "for soup in soup_objects:\n",
    "    title = soup.find('title').text\n",
    "    # Add the necessary code to extract the body from an HTML code\n",
    "    body = #FIXME\n",
    "    html_dict['Title'].append(title)\n",
    "    html_dict['Body'].append(body)\n",
    "    \n",
    "df_jobs = pd.DataFrame(html_dict)\n",
    "summary = df_jobs.describe()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve confirmed that no duplicates are present within the html. Now, lets explore the HTML content in more detail. The goal of our exploration will be to determine how job-skills are described in the HTML.\n",
    "\n",
    "### Exploring the HTML for Skill Descriptions\n",
    "\n",
    "We’ll start our exploration by rendering the HTML at index of 0 of `html_contents`.\n",
    "\n",
    "**Rendering the HTML of the first job-posting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(html_contents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subdivide `html_contents[0]` into 2 conceptually-different parts:\n",
    "\n",
    ".. An initial job summary.\n",
    ".. A list of bulleted skills required to get the job.\n",
    "\n",
    "Is the next job posting structured in a similar manner? Lets find out, by rendering `html_contents[1]`.\n",
    "\n",
    "**Listing 17. 5. Rendering the HTML of the second job-posting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(FIXME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bullet-points in `html_contents[0]` and `html_contents[1]` serve a singular purpose. They offer us brief, sentence-length descriptions of unique skills required for each position.\n",
    "\n",
    "Do these types of bulleted skill descriptions appear in other job posts? We’ll now find out. First, we’ll extract the bullets from each of our parsed html files. \n",
    "\n",
    "**Listing 17. 6. Extracting bullets from the HTML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Bullets'] = [[bullet.text.strip() \n",
    "                      for bullet in soup.find_all('li')]\n",
    "                      for soup in soup_objects]\n",
    "\n",
    "df_jobs.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What percentage of job postings actually contain bulleted text? We need to find out! If that percentage is too low, then further bullet-analysis is not worth our time.\n",
    "\n",
    "**Measuring the percent of bulleted postings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulleted_post_count = 0\n",
    "for bullet_list in df_jobs.Bullets:\n",
    "    if bullet_list:\n",
    "        bulleted_post_count += 1\n",
    "          \n",
    "percent_bulleted = 100 * bulleted_post_count / df_jobs.shape[0]\n",
    "print(f\"{percent_bulleted:.2f}% of the postings contain bullets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% of the job postings contain bullets. Do all (or most) of these bullets focus on skills? We better gage the contents of the bullet-points by printing the top-ranked words within their texts. Below, we'll we rank the words using summed TFDIF values. \n",
    "\n",
    "**Examining the top-ranked words in the HTML bullet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def rank_words(text_list):\n",
    "    vectorizer = TfidfVectorizer(#FIXME)                         # Add English Stop words\n",
    "    tfidf_matrix = vectorizer.fit_transform(#FIXME).toarray()\n",
    "    df = pd.DataFrame({'Words': vectorizer.get_feature_names_out(),\n",
    "                       'Summed TFIDF': tfidf_matrix.sum(axis=0)})\n",
    "    sorted_df = df.sort_values('Summed TFIDF', ascending=False)\n",
    "    return sorted_df\n",
    "\n",
    "all_bullets = []\n",
    "for bullet_list in df_jobs.Bullets:\n",
    "    all_bullets.extend(bullet_list)\n",
    "\n",
    "sorted_df = rank_words(all_bullets)\n",
    "print(sorted_df[:5].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terms such as _skills_ and _ability_ appear among the top 5 bulleted words. How do these bulleted words compare to the remaining words in each job posting? Lets find out. \n",
    "\n",
    "**Examining the top-ranked words in the HTML bodies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bullets = []\n",
    "for soup in soup_objects:\n",
    "    body = soup.body\n",
    "    for tag in body.find_all('li'): \n",
    "        tag.decompose()\n",
    "        \n",
    "    non_bullets.append(body.text)\n",
    "    \n",
    "sorted_df = rank_words(non_bullets)\n",
    "print(sorted_df[:5].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words _skills_ and _ability_ are no longer present in the ranked output.  Thus, the non-bulleted text appears to be less skill-oriented than the bullet contents.\n",
    "\n",
    "Strangely, the words _scientist_ and _science_ is missing from the list. Perhaps some posts pertain to data-driven jobs which aren't directly data science jobs? Lets actively explore this possibility. \n",
    "\n",
    "**Checking titles for references to data science positions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'Data Scien(ce|tist)'\n",
    "df_non_ds_jobs = df_jobs[~df_jobs.Title.str.contains(regex, case=False)]\n",
    "\n",
    "percent_non_ds = 100 * df_non_ds_jobs.shape[0] / df_jobs.shape[0]\n",
    "print(f\"{percent_non_ds:.2f}% of the job posting titles do not mention a \"\n",
    "       \"data science position. Below is a sample of such titles:\\n\")\n",
    "\n",
    "for title in df_non_ds_jobs.Title[:10]:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theirst posting on the list is for a _Patient Care Assistant_ or _PCA_. The erroneous posting contains skills that we both lack and also have no interest in obtaining.\n",
    "\n",
    "**Sampling bullets from a non-data science job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bullets = df_non_ds_jobs.Bullets.iloc[0]\n",
    "for i, bullet in enumerate(bullets[:5]):\n",
    "     print(f\"{i}: {bullet.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to leverage text similarity in order to filter out those jobs that do not align with our resume content. \n",
    "Lets proceed to filter the jobs by relevance.\n",
    "\n",
    "## Filtering Jobs by Relevance\n",
    "\n",
    "Our goal is to evaluate job relevance using text similarity. We'll want to compare the text within each posting to our resume and/or book table-of-contents. In preparation, lets now store our resume within a `resume` string.\n",
    "\n",
    "**Loading the resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = open('resume.txt', 'r').read()\n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this same manner, we can store the table of contents within a `table_of_contents` string.\n",
    "\n",
    "**Loading the table-of-content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired = open('desired_skills.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together, `resume` and `table_of_contents` summarize our existing skill-set. Lets concatenate these skills into a single `existing_skills` string.\n",
    "\n",
    "**Combining skills into a single string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_skills = resume + desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute all similarities between `df_jobs.Body` and `existing_skills.` This computation first requires that we vectorize all texts. \n",
    "\n",
    "**Vectorizing our skills and the job-posting data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df_jobs.Body.values.tolist() + [existing_skills]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(#FIXME).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectorized texts are stored in a matrix format within `tfidf_matrix`. The final matrix row (`tfidf_matrix[-1]`) corresponds to our existing skill-set. Thus, we can easily compute the cosine similarities between the job postings and `existing_skills`, by running `tfdf_matrix[:-1] @ tfidf_matrix[-1]`. \n",
    "\n",
    "**Computing skill-based cosine similarities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = tfidf_matrix[:-1] @ tfidf_matrix[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage `cosine_similarities` to rank to the jobs by relevance. Lets proceed to carry out the ranking. Afterwards, we'll confirm if the 20 least-relevant jobs have anything to do with data science.\n",
    "\n",
    "**Printing the 20 least-relevant jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs['Relevance'] = cosine_similarities\n",
    "sorted_df_jobs = df_jobs.sort_values('Relevance', ascending=False)\n",
    "for title in sorted_df_jobs[-20:].Title:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the printed jobs are completely irrelevant! Now, for comparison’s sake, lets print the 20 most-relevant job titles in `sorted_ds_jobs`.\n",
    "\n",
    "**Printing the 20 most-relevant jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in sorted_df_jobs[:20].Title:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenv`df_jobs.Relevance` is high, the associated job postings are relevant. As `df_jobs.Relevance` decreases, the associated jobs become less relevant. Thus, we can presume that there exists some `df_jobs.Relevance` cutoff, which separates the relevant jobs from the non-relevant jobs. Lets try to identify that cutoff. by visualizing the shape of the sorted relevance distribution.\n",
    "\n",
    "**Plotting job-ranking vs relevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(df_jobs.shape[0]), sorted_df_jobs.Relevance.values)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Relevance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our relevance curve resembles a K-means Elbow plot. Initially, the relevance drops rapidly. Then, at an x-value of approximately 60, the curve begins to level off. Lets emphasize this transition by striking a vertical line through the x-position of 60 in our plot.\n",
    "\n",
    "**Adding a cutoff to the relevance plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(df_jobs.shape[0]), sorted_df_jobs.Relevance.values)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Relevance')\n",
    "plt.axvline(60, c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our plot implies that the first 60 jobs are noticeably more relevant than all subsequent jobs. Based on our hypothesis, jobs 40 - 60 should be highly relevant.\n",
    "\n",
    "**Printing jobs below the relevance cutoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in sorted_df_jobs[40: 60].Title.values:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all the printed jobs are are highly relevant. Implicity, the relevance should decrease when we print the next 20 job-titles, since they lie beyond the bounds of index 60. Lets verify if this is the case.\n",
    "\n",
    "**Printing jobs beyond the relevance cutoff**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in sorted_df_jobs[60: 80].Title.values:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A majority of the printed jobs refer to data science / analyst roles, outside the scope of health science or management. We can quickly quantify this observation using regular expressions. Below, we’ll define a `percent_relevant_tiles` function, which returns the percent of non-management data science and analysis jobs within a data frame slice.\n",
    "\n",
    "**Measuring title relevance in a subset of jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def percent_relevant_titles(df):\n",
    "    regex_relevant = re.compile(r'Data (Scien|Analy)',\n",
    "                                flags=re.IGNORECASE)\n",
    "    regex_irrelevant = re.compile(r'\\b(Manage)',\n",
    "                                  flags=re.IGNORECASE)\n",
    "    match_count = len([title for title in df.Title\n",
    "                       if regex_relevant.search(title)\n",
    "                       and not regex_irrelevant.search(title)])\n",
    "    percent = 100 * match_count / df.shape[0]\n",
    "    return percent\n",
    "\n",
    "percent = percent_relevant_titles(sorted_df_jobs[60: 80])\n",
    "print(f\"Approximately {percent:.2f}% of job titles between indices \"\n",
    "       \"60 - 80 are relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 65% of the job-titles in `sorted_df_jobs[60: 80]` are relevant. Perhaps that percentage will drop if we sample the next 20 jobs, across an index range of 80 - 100. \n",
    "\n",
    "**Measuring title relevance in the next subset of jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = percent_relevant_titles(sorted_df_jobs[80: 100])\n",
    "print(f\"Approximately {percent:.2f}% of job titles between indices \"\n",
    "       \"80 - 100 are relevant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! The data science title-percentage rose to 80%. At what point will the percentage drop below 50%? We can easily find out, using a plot.\n",
    "\n",
    "**Plotting percent relevance across all title samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_title_plot(index_range=20): \n",
    "    percentages = []\n",
    "    start_indices = range(df_jobs.shape[0] - index_range) \n",
    "    for i in start_indices:\n",
    "        df_slice = sorted_df_jobs[i: i + index_range]\n",
    "        percent = percent_relevant_titles(df_slice)\n",
    "        percentages.append(percent)\n",
    "\n",
    "    plt.plot(start_indices, percentages)\n",
    "    plt.axhline(50, c='k')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('% Relevant Titles')\n",
    "\n",
    "relevant_title_plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant Data Science titles drop below 50% at an index of around 700. Of course, its possible that the cutoff of 700 is merely an artifact of our chosen index range. Will the cutoff still be present if double our index range? Lets find out.\n",
    "\n",
    "**Plotting percent relevance across an increased index-range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_title_plot(index_range=40)\n",
    "plt.axvline(700, c='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our updated plot continues to drop below 50% at an index cutoff of 700.\n",
    "\n",
    "\n",
    "At this point, we face a choice between 2 relevance cutoffs. Our first cutoff, at index 60, is highly precise. Meanwhile, our second cutoff of 700 captures many-more data science positions. So, which cutoff do we choose?  How about we try both cutoffs? That way compare the tradeoffs and benefits of each! \n",
    "\n",
    "We will now proceed to cluster the skills within the relevant job postings. Initially, we’ll set our relevance cutoff to an index of 60. \n",
    "\n",
    "## Clustering Skills in Relevant Job Postings\n",
    "\n",
    "Our aim is to cluster the skills within the 60 most-relevant job postings. The skills within each posting are diverse, and partially represented by bullet-points. Consequently, we’ll proceed to cluster the scraped bullets. We’ll start by storing `sorted_df_jobs[:60].Bullets` within a single list.\n",
    "\n",
    "**Obtaining bullets from the 60 most-relevant jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bullets = []\n",
    "for bullets in sorted_df_jobs[:60].Bullets:\n",
    "    total_bullets.extend(bullets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many bullets are present in the list? Also, are any of the bullets duplicated? We can check using Pandas.\n",
    "\n",
    "**Summarizing basic bullet statistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bullets = pd.DataFrame({'Bullet': total_bullets})\n",
    "print(df_bullets.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 900 of the bullets are unique. Below, we’ll filter empty strings and duplicates from our bullet list. Afterwards, we’ll vectorize the list using a TFIDF vectorizer.\n",
    "\n",
    "**Removing duplicates and vectorizing the bullets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bullets = sorted(set(total_bullets))\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(total_bullets)\n",
    "num_rows, num_columns = tfidf_matrix.shape\n",
    "print(f\"Our matrix has {num_rows} rows and {num_columns} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectorized TFIDF matrix is too large for efficient clustering. Lets dimensionally reduce the matrix using the procedure described in Section Fifteen. \n",
    "\n",
    "**Dimensionally reducing the TFIDF matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "np.random.seed(0)\n",
    "\n",
    "def shrink_matrix(tfidf_matrix):\n",
    "    svd_object = TruncatedSVD(n_components=100)\n",
    "    shrunk_matrix = svd_object.fit_transform(tfidf_matrix)\n",
    "    return normalize(shrunk_matrix)\n",
    "\n",
    "shrunk_norm_matrix = shrink_matrix(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nearly ready to cluster our normalized matrix using K-means. However, we’ll first need to estimate K using an Elbow plot.\n",
    "\n",
    "**Plotting an elbow curve using Mini Batch K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "def generate_elbow_plot(matrix):\n",
    "    k_values = range(1, 61)\n",
    "    inertia_values = [MiniBatchKMeans(k).fit(matrix).inertia_\n",
    "                      for k in k_values]\n",
    "    plt.plot(k_values, inertia_values)\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "generate_elbow_plot(shrunk_norm_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precise location of a bent elbow-shaped transition is difficult to spot. We don't know which K-value to choose. Lets cluster our data multiple times, using K-values of 10, 15, 20, and 25. Afterwards, we’ll compare and contrast the results.\n",
    "\n",
    "### Grouping the Job Skills into 15 Clusters\n",
    "\n",
    "Below, we’ll execute K-means using a K of 15. \n",
    "\n",
    "**Clustering bullets into 15 clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def compute_cluster_groups(shrunk_norm_matrix, k=15):\n",
    "    cluster_model = #FIXME    Use KMeans to cluster\n",
    "    clusters = cluster_model.fit_predict(FIXME)  # predict\n",
    "    df = pd.DataFrame({'Index': range(clusters.size), 'Cluster': clusters,\n",
    "                       'Bullet': total_bullets})\n",
    "    return [df_cluster for  _, df_cluster in df.groupby('Cluster')]\n",
    "\n",
    "cluster_groups = compute_cluster_groups(shrunk_norm_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our text clusters is stored as a Pandas table within the `cluster_groups` list. We can visualize the clusters using word clouds. Below, we will define a `cluster_to_image` function, and then apply it to `cluster_groups[0]`.\n",
    "\n",
    "**Visualizing the first cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "np.random.seed(0)\n",
    "\n",
    "def cluster_to_image(df_cluster, max_words=10, tfidf_matrix=tfidf_matrix,\n",
    "                     vectorizer=vectorizer): \n",
    "    indices = df_cluster.Index.values\n",
    "    summed_tfidf = np.asarray(tfidf_matrix[indices].sum(axis=0))[0]\n",
    "    data = {'Word': vectorizer.get_feature_names_out(),'Summed TFIDF': summed_tfidf}\n",
    "    df_ranked_words = pd.DataFrame(data).sort_values('Summed TFIDF', ascending=False)\n",
    "    words_to_score = {word: score\n",
    "                     for word, score in df_ranked_words[:max_words].values\n",
    "                     if score != 0}\n",
    "    cloud_generator = WordCloud(background_color='white',\n",
    "                                color_func=_color_func,\n",
    "                                random_state=1)\n",
    "    wordcloud_image = cloud_generator.fit_words(words_to_score)\n",
    "    return wordcloud_image\n",
    "\n",
    "def _color_func(*args, **kwargs):\n",
    "    return np.random.choice(['black', 'blue', 'teal', 'purple', 'brown'])\n",
    "\n",
    "wordcloud_image = cluster_to_image(cluster_groups[0])\n",
    "plt.imshow(wordcloud_image, interpolation=\"bilinear\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language in the word-cloud appears to be describing a focused, data-oriented personality. However, that language is a little vague. Perhaps we can learn more about the cluster by printing some sample bullets from `cluster_group[0]`.\n",
    "\n",
    "**Printing sample bullets from Cluster 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "def print_cluster_sample(cluster_id):\n",
    "    df_cluster = cluster_groups[cluster_id]\n",
    "    for bullet in np.random.choice(df_cluster.Bullet.values, 5, \n",
    "                                   replace=False):\n",
    "        print(bullet)\n",
    "\n",
    "print_cluster_sample(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets visualize all 15 clusters using word clouds. \n",
    "\n",
    "**Visualizing all 15 clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud_grid(cluster_groups, num_rows=5, num_columns=3,\n",
    "                        **kwargs):\n",
    "    figure, axes = plt.subplots(num_rows, num_columns, figsize=(20, 15))\n",
    "    cluster_groups_copy = cluster_groups[:]\n",
    "    for r in range(num_rows):\n",
    "        for c in range(num_columns):\n",
    "            if not cluster_groups_copy:\n",
    "                break\n",
    "                \n",
    "            df_cluster = cluster_groups_copy.pop(0)\n",
    "            wordcloud_image = cluster_to_image(df_cluster, **kwargs)\n",
    "            ax = axes[r][c]\n",
    "            ax.imshow(wordcloud_image,\n",
    "            interpolation=\"bilinear\")   \n",
    "            ax.set_title(f\"Cluster {df_cluster.Cluster.iloc[0]}\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "\n",
    "plot_wordcloud_grid(cluster_groups)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the clusters are highly technical. For instance, Cluster 7 fixates on external data science libraries such as Scikit-Learn, Pandas, NumPy, Matplotlib and SciPy. Lets print sample some bullets from Cluster 7, and confirm their focus on DS libraries.\n",
    "\n",
    "**Printing sample bullets from Cluster 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "print_cluster_sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, other clusters focus on non-technical skills. Perhaps we can separate the technical clusters and soft-skill clusters using text similarity (to our resume). Lets examine this possibility. We’ll start by computing the cosine similarity between each bullet in `total_bullets` and our resume.\n",
    "\n",
    "**Computing similarities between the bullets and our resume**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bullet_similarity(bullet_texts):\n",
    "    bullet_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    matrix = bullet_vectorizer.fit_transform(bullet_texts + [resume])\n",
    "    matrix = matrix.toarray()\n",
    "    return matrix[:-1] @ matrix[-1]\n",
    "\n",
    "bullet_cosine_similarities = compute_bullet_similarity(total_bullets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `bullet_cosine_similarities` array contains the text similarities across all clustered bullets. For any given cluster, we can combine these cosine similarities into a single similarity score. According to our hypothesis, a technical cluster should have a higher mean similarity than a soft-skill similarity cluster. Lets confirm if this is the case for the technical Cluster 7 and the soft-skill Cluster 0.\n",
    "\n",
    "**Comparing mean resume similarities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_similarity(df_cluster):\n",
    "    indices = df_cluster.Index.values\n",
    "    return bullet_cosine_similarities[indices].mean()\n",
    "\n",
    "tech_mean = compute_mean_similarity(cluster_groups[7])\n",
    "soft_mean =  compute_mean_similarity(cluster_groups[0])\n",
    "print(f\"Technical cluster 7 has a mean similarity of {tech_mean:.3f}\")\n",
    "print(f\"Soft-skill cluster 3 has a mean similarity of {soft_mean:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical cluster is 10x more proximate to our resume than the soft-skill cluster. Lets compute the average similarity for all 15 clusters. Afterwards, we’ll sort the clusters by their similarity score, in descending order.\n",
    "\n",
    "**Sorting subplots by resume similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_cluster_groups(cluster_groups):\n",
    "    mean_similarities = [compute_mean_similarity(df_cluster)\n",
    "                         for df_cluster in cluster_groups]\n",
    "    \n",
    "    sorted_indices = sorted(range(len(cluster_groups)),\n",
    "                            key=lambda i: mean_similarities[i],\n",
    "                            reverse=True)\n",
    "    return [cluster_groups[i] for i in sorted_indices]\n",
    "\n",
    "sorted_cluster_groups = sort_cluster_groups(cluster_groups)\n",
    "plot_wordcloud_grid(sorted_cluster_groups)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis was right! The first 2 rows within the updated subplot clearly correspond to technical skills. F\n",
    "\n",
    "### Investigating the Technical Skill Clusters\n",
    "Lets turn our attention to the 6 technical-skill cluster within first 2 rows of subplot grid. Below, we’ll re-plot their associated word-clouds in a 2-row by 3-column grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud_grid(sorted_cluster_groups[:6], num_rows=3, num_columns=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 rows are insightful. We'll need to brush up on our machine learning skills. However, the final 2 technical-skill clusters appear are vague and uninformative. Below, we’ll sample bullets from these clusters (8 and 1) in order to confirm a lack of pattern.\n",
    "\n",
    "**Printing sample bullets from Clusters 8 and 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "for cluster_id in [8, 1]:\n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print_cluster_sample(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve finished our analysis of the technical-skill clusters. 4 of these clusters were relevant. 2 were not. Now, lets turn our attention to the remaining soft-skill clusters.\n",
    "\n",
    "### Investing the Soft-Skill Clusters\n",
    "We’ll start by visualizing the remaining 9 soft-skill clusters in a 3-row by 3-column grid.\n",
    "\n",
    "**Plotting the remaining 9 soft-skill clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud_grid(sorted_cluster_groups[6:], num_rows=3, num_columns=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining clusters appear abmbiguous. Also, our soft-skill clusters do not represent true skills! For instance, Cluster 3 (row 0, column 0) consists of bullets requiring a minimal count of years working in industry. Similarly, Cluster 6 (row 2, column 1) represents educational constraints; requiring a quantitative degree to land an interview. \n",
    "\n",
    "**Printing sample bullets from Clusters 6 and 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "for cluster_id in [6, 3]:\n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print_cluster_sample(cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one of our soft-skill clusters is very easy to interpret. Cluster 5 (row 0, column 1) focuses on interpersonal communication skills; both written and verbal. \n",
    "\n",
    "\n",
    "### Exploring Clusters at Alternative Values of K\n",
    "\n",
    "Lets we’ll regenerate the clusters using alternative values of K. We’ll begin by setting K to 25, and plotting the results in a 5-row by 5-column grid. The subplots will be sorted based on cluster similarity to our resume. \n",
    "\n",
    "**Visualizing 25 sorted clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "cluster_groups = compute_cluster_groups(shrunk_norm_matrix, k=25)\n",
    "sorted_cluster_groups = sort_cluster_groups(cluster_groups)\n",
    "plot_wordcloud_grid(sorted_cluster_groups, num_rows=5, num_columns=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raising K from 15 to 25 has retained all previously-observed insightful clusters. Also, the parameter change has introduced several new interesting clusters (Including Cluster 2: Non-Python Visualization tools and Cluster 23; Databases) Will the stability of these clusters persist if we shift K to an intermediate value of 20? Lets find out.\n",
    "\n",
    "**Visualizing 20 sorted clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "cluster_groups = compute_cluster_groups(shrunk_norm_matrix, k=20)\n",
    "sorted_cluster_groups = sort_cluster_groups(#FIXME)\n",
    "plot_wordcloud_grid(sorted_cluster_groups, num_rows=4, num_columns=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our observed insightful clusters remain at `k=20`. However, the statistical analysis cluster observed at K-values of 15 and 25 is currently missing. If we cluster over just a single value of K, we risk missing out on useful insights. Thus, it’s preferable to visualize results over a range of K-values during text analysis. With this in mind, lets see what happens when we drop K to 10.\n",
    "\n",
    "**Visualizing 10 sorted clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "cluster_groups = compute_cluster_groups(shrunk_norm_matrix, k=10)\n",
    "sorted_cluster_groups = sort_cluster_groups(cluster_groups)\n",
    "plot_wordcloud_grid(sorted_cluster_groups, num_rows=5, num_columns=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 10 visualized clusters are quite limited. Nonetheless, 4 of the 10 clusters contain the critical skills we’ve previously observed. These include Python programming (row 0, column 0), machine learning (row 0, column 1), and communication skills (row 2, column 1). Also, the statistical analysis cluster has reappered (row 1, column 0).\n",
    "Surprisingly, some of our skill-clusters are quite versatile. \n",
    "\n",
    "So far, our observations have been limited to the 60 most relevant job postings. What will happen if we extand our analysis to the top 700 postings?  We’ll now find out.\n",
    "\n",
    "### Analyzing the 700 Most-Relevant Postings\n",
    "\n",
    "We’ll start by preparing `sorted_df_jobs[:700].Bullets` for clustering.\n",
    "\n",
    "**`Preparing sorted_df_jobs[:700]` for clustering analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "total_bullets_700 = set()\n",
    "for bullets in sorted_df_jobs[:700].Bullets:\n",
    "    total_bullets_700.update([bullet.strip()\n",
    "                              for bullet in bullets])\n",
    "\n",
    "total_bullets_700 = sorted(total_bullets_700)\n",
    "vectorizer_700 = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_700 = vectorizer_700.fit_transform(#FIXME)\n",
    "shrunk_norm_matrix_700 = shrink_matrix(#FIXME)\n",
    "print(f\"We've vectorized {shrunk_norm_matrix_700.shape[0]} bullets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve vectorized 10,194 bullet-points. Now, we’ll generate an Elbow plot across the vectorized results. Based on previous observations, we don’t expect the Elbow plot to be particularly informative. \n",
    "\n",
    "**Plotting an elbow curve for 10,194 bullets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "generate_elbow_plot(shrunk_norm_matrix_700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the precise location of the elbow is not clear within the plot. The elbow is spread out between a K of 10 and 25. We’ll deal with ambiguity, by arbitrarily setting K to 20.\n",
    "\n",
    "**WARNING: As we discussed in Section Fifteen, the K-means outputs could vary across computers for large matrices containing 10,000-by-100 elements. Thus, your local clustering results might differ from the output seen below. Nonetheless, you should be able to draw similar conclusions to the ones presented in this book.**\n",
    "\n",
    "**Visualizing 20 sorted clusters for 10,194 bullets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "cluster_groups_700 = compute_cluster_groups(shrunk_norm_matrix_700, k=20)\n",
    "bullet_cosine_similarities = compute_bullet_similarity(total_bullets_700)\n",
    "sorted_cluster_groups_700 = sort_cluster_groups(cluster_groups_700)\n",
    "plot_wordcloud_grid(sorted_cluster_groups_700, num_rows=4, num_columns=5,\n",
    "                    vectorizer=vectorizer_700, tfidf_matrix=tfidf_matrix_700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our clustering output looks very similar to what we’ve seen before. The key insightful clusters we’ve observed at 60 postings still remain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
